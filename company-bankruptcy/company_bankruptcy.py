# -*- coding: utf-8 -*-
"""company_bankruptcy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1S8uqHGLm8Za9rHmHJ525sBNZ3HN04sTL

# Exploratory Data Analysis
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/company_bankruptcy_prediction.csv')
pd.set_option('display.max_columns', None)

df

# Calculate the number of null values per column
null_counts = df.isnull().sum()

# Check if there are any null values
if null_counts.any():
    print(null_counts[null_counts > 0])

# Remove null values
df.dropna(inplace=True)

# Drop constant features
constant_features = df.columns[df.nunique() == 1]
print(constant_features)
df.drop(constant_features, axis=1, inplace=True)

df

df.columns

# Loop through and print column names and dtypes in chunks
for col_name, dtype in df.dtypes.items():
    print(f"{col_name}: {dtype}")

"""# Visualizations"""

# TODO: Add data visualizations here

"""# Data Preprocessing"""

from imblearn.over_sampling import SMOTE

# Handling imbalanced classes
X = df.drop('Bankrupt?', axis=1)
y = df['Bankrupt?']
oversample = SMOTE()
X_resampled, y_resampled = oversample.fit_resample(X, y)

from sklearn.preprocessing import StandardScaler

# Feature scaling
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_resampled)

"""# Feature Selection"""

# TODO: Add feature selection here

"""# Model Selection and Training"""

from sklearn.model_selection import train_test_split

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_resampled, test_size=0.2, random_state=42)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {'n_estimators': [100, 200, 300],
              'max_depth': [None, 10, 20],
              'min_samples_split': [2, 5, 10]}

rf = RandomForestClassifier(random_state=42)
grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='roc_auc')
grid_search.fit(X_train, y_train)

# Best parameters and model
best_rf = grid_search.best_estimator_

from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report

# Make predictions
y_pred = best_rf.predict(X_test)
y_pred_proba = best_rf.predict_proba(X_test)[:, 1]

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
roc_auc = roc_auc_score(y_test, y_pred_proba)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"ROC AUC Score: {roc_auc}")
print("Confusion Matrix:")
print(conf_matrix)
print("Classification Report:")
print(class_report)

import joblib

# Save the best model
joblib.dump(best_rf, 'best_random_forest_model.pkl')
# -*- coding: utf-8 -*-
"""psychosocial_dimensions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HjQt8gJmt7eJpxr6E20-V4mXYtkEWf8P

# Exploratory Data Analysis
"""

import pandas as pd

# Load the dataset
df = pd.read_csv('/content/CSE_student_performances.csv')

df

# Check for missing values
print(df.isnull().sum())

# Replacing the missing values with mean
mean_value = df['NumberOfFriend'].mean()
df['NumberOfFriend'].fillna(mean_value, inplace=True)

print(df.isnull().sum())

print(df.describe(include='all'))

df.columns

# Remove any trailing spaces
df.columns = df.columns.str.strip()

df.columns

"""# Visualizations"""

import seaborn as sns
import matplotlib.pyplot as plt

# Pairplot to see relationships
sns.pairplot(df, hue='DepressionStatus')
plt.show()

numeric_df = df.select_dtypes(include=[float, int])

# Correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm')
plt.show()

# Distribution of numerical features
df.hist(bins=30, figsize=(15, 10))
plt.show()

# Countplot for categorical features
categorical_features = ['Gender', 'AcademicPerformance', 'TakingNoteInClass', 'FaceChallangesToCompleteAcademicTask', 'LikePresentation', 'LikeNewThings']
for feature in categorical_features:
    sns.countplot(data=df, x=feature, hue='DepressionStatus')
    plt.show()

"""# Data Preprocessing"""

df.dtypes

from sklearn.preprocessing import LabelEncoder

# categorical_features = ['Gender', 'AcademicPerformance', 'TakingNoteInClass', 'FaceChallangesToCompleteAcademicTask', 'LikePresentation', 'LikeNewThings']

# Encode categorical variables
label_encoders = {}
for column in df.select_dtypes(include=['object']).columns:
    le = LabelEncoder()
    df[column] = le.fit_transform(df[column])
    label_encoders[column] = le

df

df.dtypes

print(df.columns)

from sklearn.preprocessing import StandardScaler

# Standardize numerical features
scaler = StandardScaler()
numerical_features = ['Age', 'SleepPerDayHours', 'NumberOfFriend']
df[numerical_features] = scaler.fit_transform(df[numerical_features])

df

"""# Feature Selection"""

# Get correlation of features with target variable
correlation = df.corr()
print(correlation['DepressionStatus'].sort_values(ascending=False))

from sklearn.ensemble import RandomForestClassifier

# RandomForest for feature importance
X = df.drop('DepressionStatus', axis=1)
y = df['DepressionStatus']

model = RandomForestClassifier()
model.fit(X, y)

feature_importances = pd.DataFrame(model.feature_importances_,
                                   index=X.columns,
                                   columns=['importance']).sort_values('importance', ascending=False)
print(feature_importances)

"""# Model Selection and Training"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Multiple Models
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier

models = {
    'Logistic Regression': LogisticRegression(),
    'Support Vector Machine': SVC(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    print(f'{name} trained.')

"""# Evaluation"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

for name, model in models.items():
    y_pred = model.predict(X_test)
    print(f'{name} Accuracy: {accuracy_score(y_test, y_pred)}')
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))

"""# Pipeline Implementation"""

from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# Preprocessing for numerical data
numerical_features = ['Age', 'SleepPerDayHours', 'NumberOfFriend']
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Preprocessing for categorical data
categorical_features = ['Gender', 'AcademicPerformance', 'TakingNoteInClass', 'FaceChallangesToCompleteAcademicTask', 'LikePresentation', 'LikeNewThings']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combine preprocessing steps
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# Create the pipeline
clf = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', GradientBoostingClassifier())
])

# Train the pipeline
clf.fit(X_train, y_train)

# Predict and evaluate
y_pred = clf.predict(X_test)
print(f'Pipeline Model Accuracy: {accuracy_score(y_test, y_pred)}')
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))